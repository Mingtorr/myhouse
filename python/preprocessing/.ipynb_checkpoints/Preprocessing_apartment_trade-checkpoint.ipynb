{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.error import URLError, HTTPError\n",
    "from http.client import IncompleteRead\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kakao api\n",
    "APP_KEY = \"\"\n",
    "kakao_url = \"https://dapi.kakao.com/v2/local/search/address.json\"\n",
    "kakao_headers = {\"Authorization\" : \"KakaoAK {}\".format(APP_KEY)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kakao_req(address):\n",
    "#     print('HTTP Method: %s' % method)\n",
    "#     print('Request URL: %s' % kakao_url)\n",
    "#     print('Headers: %s' % kakao_headers)\n",
    "#     print('QueryString: %s' % params)\n",
    "    kakao_url_string = str(kakao_url)+ '?query='+ str(address)\n",
    "    try:\n",
    "        resultXML = requests.get(kakao_url_string, headers=kakao_headers)\n",
    "    except HTTPError as e:\n",
    "        print(\"502 error caused reply\" + e)\n",
    "        resultXML = requests.get(kakao_url_string, headers=kakao_headers)\n",
    "    except IncompleteRead:\n",
    "        # Oh well, reconnect and keep trucking\n",
    "        print(\"_chunk_size Error\")\n",
    "        resultXML = requests.get(kakao_url_string, headers=kakao_headers)\n",
    "    except URLError as e:\n",
    "        print(e.reason)\n",
    "        resultXML = requests.get(kakao_url_string, headers=kakao_headers)\n",
    "    else:\n",
    "        data= resultXML.json()\n",
    "        if len(data['documents']) ==0:\n",
    "            data= '403'\n",
    "            return data\n",
    "        else:\n",
    "            return resultXML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(address):\n",
    "\n",
    "    params = {\"query\": \"{}\".format(address)}\n",
    "\n",
    "    response = kakao_req(params, 'GET')\n",
    "        \n",
    "\n",
    "#    #kakao parsing\n",
    "#     pprint(data['meta']['total_count'])\n",
    "#     pprint(data[\"documents\"][0][\"x\"])\n",
    "#     pprint(data[\"documents\"][0][\"y\"])\n",
    "    \n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apartment_trade_data_all.csv\n"
     ]
    }
   ],
   "source": [
    "basic_folder = './apartment_trade_data/'\n",
    "\n",
    "file_list = os.listdir(basic_folder)\n",
    "path_list=[]\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    path_list.append(basic_folder + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = df.na.drop()\n",
    "# drop the rows containing any null or NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_date_df = filter_df.groupBy(filter_df.year,filter_df.month, filter_df.day).agg(count(\"day\").alias('count'))\n",
    "# agg = exprs에 들어있는 계산식으로 dataframe 계산\n",
    "\n",
    "unique_date_list = unique_date_df.collect()\n",
    "# df 안의 모든 row 반환\n",
    "new_date_list = []\n",
    "for unique_date in unique_date_list:\n",
    "    #print(unique_date[2])\n",
    "    \n",
    "    if unique_date[1] < 10 :\n",
    "        if unique_date[2] < 10 : \n",
    "            date = str(unique_date[0]) + '-' + \"0\" + str(unique_date[1]) + '-' + \"0\" +str(unique_date[2]) + 'T' + \"00:00:00\"\n",
    "        else:\n",
    "            date = str(unique_date[0]) + '-' + \"0\" + str(unique_date[1]) + '-' +str(unique_date[2]) + 'T' + \"00:00:00\"\n",
    "        \n",
    "    else:\n",
    "        if unique_date[2] < 10 : \n",
    "            date = str(unique_date[0]) + '-' + str(unique_date[1]) + '-' + \"0\" +str(unique_date[2]) + 'T' + \"00:00:00\"\n",
    "    \n",
    "        else:\n",
    "            date = str(unique_date[0]) + '-' + str(unique_date[1]) + '-' +str(unique_date[2]) + 'T' + \"00:00:00\"\n",
    "    \n",
    "    \n",
    "    new_date_list.append([unique_date[0], unique_date[1],unique_date[2],date])\n",
    "    \n",
    "unique_date_df = sc.parallelize(new_date_list).toDF(['year','month','day','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_date_df = filter_df.join(unique_date_df, ['year','month','day']).sort('date')\n",
    "added_date_df.show()\n",
    "added_date_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import UserDefinedFunction\n",
    "\n",
    "#added_date_df.replace('[북구]','-',regex=True,)\n",
    "#print(added_date_df)\n",
    "#udf = UserDefinedFunction(lambda x: x.replace(\"북구\",\".\"), StringType())\n",
    "added_date_df = added_date_df.withColumn(\"detailed_address\", regexp_replace(col(\"detailed_address\"),\"=\",\"-\"))\n",
    "added_date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = added_date_df.groupBy(added_date_df.address,added_date_df.detailed_address).agg(count(\"detailed_address\").alias(\"count\"))\n",
    "new_df.show()\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = new_df.select(sum(\"count\"))\n",
    "sum.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_address_list = new_df.collect()\n",
    "#print (unique_address_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def req_addr(address):\n",
    "    data=0\n",
    "    if '-' in address[1]:\n",
    "        check = str(address[1]).split('-')[0]\n",
    "        if address[1].split('-')[0].isdigit() == False or address[1].split('-')[1].isdigit() == False:\n",
    "            print(address)\n",
    "            print('주소에 글자섞임')\n",
    "            data= 400\n",
    "    if data == 0:\n",
    "        response = kakao_req(address[0] + \" \" + address[1])\n",
    "    else :\n",
    "        return data\n",
    "    if response == '403':\n",
    "        print('빈배열 에러 ' + address[0] + \" \" + address[1])\n",
    "        sleep(5)\n",
    "        data= 3\n",
    "        return data\n",
    "    else :\n",
    "        return response\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "location_list = []\n",
    "\n",
    "# checking why no esult data\n",
    "# response = get_location(\"경상북도 영천시 신령면 완전리\") -> 신녕면\n",
    "# print response.status_code\n",
    "# data = response.json()\n",
    "# print data\n",
    "\n",
    "index = 1.0;\n",
    "length = len(unique_address_list)\n",
    "count = 0;\n",
    "print (length)\n",
    "addr2 =unique_address_list[:1]\n",
    "\n",
    "for address in unique_address_list: \n",
    "    #full_address = (address[0] + \" \" + address[1]).encode('utf8')\n",
    "    #print(full_address)\n",
    "    response = req_addr(address)\n",
    "    if response == 400 or response == 3:\n",
    "        continue\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        x = data['documents'][0]['address']['x']\n",
    "        y = data['documents'][0]['address']['y']\n",
    "        location_list.append([address[0],address[1],y,x])\n",
    "    else: \n",
    "        print (response.json())\n",
    "        count += 1\n",
    "        #print (count)\n",
    "        location_list.append([address[0],address[1] ,np.nan,np.nan])\n",
    "    \n",
    "    if index % 100 == 0:\n",
    "        print ((\"%0.2f\" % (index / length * 100.0)) + \"% completed\")\n",
    "        \n",
    "    index += 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = sc.parallelize(location_list).toDF(['address','detailed_address','latitude','longitude'])\n",
    "location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = added_date_df.join(location.select('address','detailed_address','latitude','longitude'), ['address','detailed_address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.show()\n",
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_index_df = joined_df.drop(\"_c0\")\n",
    "drop_index_df.show()\n",
    "drop_index_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_apartment_df = drop_index_df.na.drop()\n",
    "final_apartment_df = final_apartment_df.withColumn('trade_price',regexp_replace('trade_price',\"\\\\,\", \"\"))\n",
    "final_apartment_df.show()\n",
    "final_apartment_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_apartment_df.write.format('com.databricks.spark.csv').save('mycsv.csv')\n",
    "final_apartment_df.coalesce(1).write.csv('mycsv2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
